{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import Libraries Used in Data Preparation\n",
    "import pandas as pd\n",
    "import os\n",
    "debug = True\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Load Data from files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#Code to Load Data in a DataFrame\n",
    "datafolder = '../data/'\n",
    "#print(os.listdir(datafolder))\n",
    "train_features_file_name = 'train_features.csv'\n",
    "train_labels_file_name = 'train_salaries.csv'\n",
    "test_features_file_name = 'test_features.csv'\n",
    "\n",
    "# with open(os.path.join(datafolder, test_features_file_name)) as f:\n",
    "#     x = f.readlines()\n",
    "# print(x[1])\n",
    "train_data_features = pd.read_csv(os.path.join(datafolder, train_features_file_name))\n",
    "org_features = train_data_features.columns.tolist()\n",
    "train_data_labels = pd.read_csv(os.path.join(datafolder, train_labels_file_name))\n",
    "train_full = pd.merge(train_data_features, train_data_labels, on='jobId',how ='inner')\n",
    "test_data_features = pd.read_csv(os.path.join(datafolder, test_features_file_name))\n",
    "\n",
    "assert(train_full.shape[0] == train_data_features.shape[0])\n",
    "assert(train_full.shape[0] == train_data_labels.shape[0])\n",
    "#train_full.dropna(how ='any', inplace = True)\n",
    "#train_full.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Clean/Check Data (Pending)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "jobId                  0\n",
       "companyId              0\n",
       "jobType                0\n",
       "degree                 0\n",
       "major                  0\n",
       "industry               0\n",
       "yearsExperience        0\n",
       "milesFromMetropolis    0\n",
       "salary                 0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check train Data for any null values\n",
    "train_full.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "jobId                  0\n",
       "companyId              0\n",
       "jobType                0\n",
       "degree                 0\n",
       "major                  0\n",
       "industry               0\n",
       "yearsExperience        0\n",
       "milesFromMetropolis    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check Test Data for any null values\n",
    "test_data_features.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** No null values found so no data cleaning performed **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  1.3 Encode Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create copies of original train and test data\n",
    "train_full_encoded = train_full\n",
    "test_data_features_encoded = test_data_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 1.3.1 One Hot Encoding : Done for *companyId*, *major*, and *industry* as these variabes are categories and may influence the salary **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['OH_companyId', 'OH_major', 'OH_industry']\n"
     ]
    }
   ],
   "source": [
    "#Encode Categorical Variables One Hot (both test and train)\n",
    "\n",
    "def one_hot_encoding(c, df):\n",
    "    new_f_name = 'OH_'+ c\n",
    "    df = pd.concat([df, pd.get_dummies(df[c], prefix= new_f_name)], axis=1)\n",
    "    return df, new_f_name\n",
    "\n",
    "cat_features_normal = ['companyId', 'major', 'industry']\n",
    "cat_features_normal_new = []\n",
    "for c in cat_features_normal:\n",
    "    train_full_encoded, new_f_name = one_hot_encoding(c, train_full_encoded)\n",
    "    test_data_features_encoded, new_f_name_test = one_hot_encoding(c, test_data_features_encoded)\n",
    "    cat_features_normal_new.append(new_f_name)\n",
    "    \n",
    "    \n",
    "print(cat_features_normal_new)\n",
    "#train_full.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 1.3.2 Ordinal  Encoding : Done for *degree* and *jobType* as these variabes have an inherent order associated with them **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ORD_jobType', 'ORD_degree']\n"
     ]
    }
   ],
   "source": [
    "#Encode Categorical Variables Ordered (both test and train)\n",
    "\n",
    "def ordinal_encoding(c, df, lookup):\n",
    "    new_f_name = 'ORD_'+ c\n",
    "    df[new_f_name] =  df[c].map(lookup)\n",
    "    return df, new_f_name\n",
    "\n",
    "degreeOrder = {'BACHELORS':2, 'DOCTORAL':4, 'HIGH_SCHOOL':1, 'MASTERS':3, 'NONE':0}\n",
    "jobTypeOrder = {'CEO':7, 'CFO':6, 'CTO':5, 'VICE_PRESIDENT':4, 'MANAGER':3,'SENIOR':2, 'JUNIOR':1, 'JANITOR':0}\n",
    "\n",
    "cat_features_ordinal = ['jobType', 'degree']\n",
    "cat_features_ordinal_new = []\n",
    "for c in cat_features_ordinal:\n",
    "    train_full_encoded, new_f_name = ordinal_encoding(c, train_full_encoded, eval(c+'Order'))\n",
    "    test_data_features_encoded, new_f_name_test = ordinal_encoding(c, test_data_features_encoded, eval(c+'Order'))\n",
    "    cat_features_ordinal_new.append(new_f_name)\n",
    "\n",
    "print(cat_features_ordinal_new)\n",
    "#train_full.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Shuffle Data and Split into K folds which are used during model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Shuffle Train Data\n",
    "train_full_encoded =  train_full_encoded.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split Data into K Folds\n",
    "from sklearn.model_selection import KFold\n",
    "def GetKFoldData(df,k):\n",
    "    folds = {}\n",
    "    kfolds = KFold(n_splits=k, shuffle = True, random_state = 4) \n",
    "    foldidx = 0\n",
    "    for train_idx, test_idx in kfolds.split(df.index):\n",
    "        folds[foldidx] = { 'train': df.iloc[train_idx], 'test':df.iloc[test_idx]}\n",
    "        foldidx+=1\n",
    "    return folds\n",
    "\n",
    "#exp_data_k_folds = GetKFoldData(train_full, 10)\n",
    "#idx_list_org = exp_data_k_folds[0]['test'].index.tolist()\n",
    "#exp_data_k_folds[0]['test'].shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Set Scaler to be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use Standard Scaler\n",
    "scalermain = preprocessing.StandardScaler()\n",
    "#scalermain = preprocessing.MinMaxScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Decide Features to Use and Models to Evaluate (Feature Scaling Pending)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 2.1.1 Features to be Used (Filtered out non-encoded variables). Also marked out features which would be scaled **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Select Features\n",
    "non_cat_features = ['yearsExperience', 'milesFromMetropolis']\n",
    "cat_features =[]\n",
    "for f in train_full_encoded.columns:\n",
    "    if 'OH' in f or 'ORD' in f:\n",
    "        cat_features.append(f)\n",
    "        \n",
    "#train_full[train_features].head()\n",
    "all_features = non_cat_features + cat_features \n",
    "features_to_scale = non_cat_features + cat_features_ordinal_new\n",
    "#train_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 2.1.2 Regression Models Considered **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Different Regression Models Evaluated\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn import linear_model\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "\n",
    "models = {\n",
    "            'LR' :    {'modelobj' :  linear_model.LinearRegression(), 'MSE':None, 'RMSE':None,\\\n",
    "                                     'Avg_Training_Time':None,'Avg_Testing_Time':None},\n",
    "    \n",
    "            'Ridge' : {'modelobj' :  linear_model.Ridge(), 'MSE':None, 'RMSE':None,\\\n",
    "                                     'Avg_Training_Time':None,'Avg_Testing_Time':None,\\\n",
    "                                     'params' : {'alpha': [i for i in range(1,81,3)]}},\n",
    "    \n",
    "            'DT' :    {'modelobj' :  DecisionTreeRegressor(random_state=10), 'MSE':None, 'RMSE':None, \\\n",
    "                                     'Avg_Training_Time':None,'Avg_Testing_Time':None, \\\n",
    "                                     'params' : {\n",
    "                                                 'max_depth':[i for i in range(9,12)], \\\n",
    "                                                 'min_samples_leaf' :[i for i in range(10,21,5)]\n",
    "                                                }}#,   \n",
    "    \n",
    "#             'RF' :    {'modelobj' :  RandomForestRegressor(random_state=10), 'MSE':None, 'RMSE':None, \\\n",
    "#                                      'Avg_Training_Time':None,'Avg_Testing_Time':None, \\\n",
    "#                                      'params' : {\n",
    "#                                                  'n_estimators':[i for i in range(300,501,100)],\n",
    "#                                                  'max_depth':[i for i in range(9,12)], \n",
    "#                                                  'min_samples_leaf' :[i for i in range(10,21,5)],\n",
    "#                                                  'max_features' : ['log2', 'sqrt']\n",
    "#                                                  }},\n",
    "    \n",
    "#               'GBR' :    {'modelobj' :   GradientBoostingRegressor(), 'MSE':None, 'RMSE':None, \\\n",
    "#                                      'Avg_Training_Time':None,'Avg_Testing_Time':None, \\\n",
    "#                                      'params' : {}}\n",
    "                                                 \n",
    "   \n",
    "    \n",
    "         \n",
    "         }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Select Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "code_folding": [
     0,
     5,
     36
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Helper Functions to Evaluate a Model given train and test data\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import time\n",
    "\n",
    "def EvalModelSplitData(m, model, curr_fold, all_features, features_to_scale, \\\n",
    "                       scaler = preprocessing.StandardScaler()):\n",
    "    \n",
    "    #Train Data Selection\n",
    "    curr_fold_X_train = curr_fold['train'][all_features]\n",
    "    curr_fold_X_train_scaled= curr_fold_X_train\n",
    "    curr_fold_X_train_scaled[features_to_scale] = scaler.fit_transform(curr_fold_X_train[features_to_scale])\n",
    "    curr_fold_y_train = curr_fold['train']['salary'] \n",
    "    \n",
    "    tr_st = time.time()\n",
    "    curr_model = model['modelobj'].fit(curr_fold_X_train_scaled, curr_fold_y_train)\n",
    "    tr_time = time.time() - tr_st\n",
    "    \n",
    "    \n",
    "    #Test\n",
    "    curr_fold_X_test = curr_fold['test'][all_features]\n",
    "    curr_fold_X_test_scaled = curr_fold_X_test\n",
    "    curr_fold_X_test_scaled[features_to_scale] = scaler.fit_transform(curr_fold_X_test[features_to_scale])\n",
    "    curr_fold_y_test = curr_fold['test']['salary']\n",
    "    \n",
    "    tst_st = time.time()\n",
    "    curr_test_predict = curr_model.predict(curr_fold_X_test_scaled)\n",
    "    tst_time = time.time() - tst_st\n",
    "    \n",
    "    #Compute Cost (MSE/RMSE)\n",
    "    curr_fold_mse = MSE(curr_test_predict, curr_fold_y_test)\n",
    "    curr_fold_rmse = curr_fold_mse**0.5\n",
    "    \n",
    "    return curr_fold_mse, curr_fold_rmse, tr_time, tst_time\n",
    "    \n",
    "\n",
    "def GetBestParams(m, model, alldata, all_features, features_to_scale, k,\\\n",
    "                  scaler = preprocessing.StandardScaler()):\n",
    "                  \n",
    "    X_train = alldata[all_features]\n",
    "    y_train = alldata['salary']\n",
    "    X_train_scaled = X_train\n",
    "    X_train_scaled[features_to_scale] = scaler.fit_transform(X_train[features_to_scale])\n",
    "    \n",
    "    \n",
    "    if m in ['Ridge','DT']:\n",
    "        model_search = GridSearchCV(model['modelobj'], model['params'], cv=k, verbose=0, n_jobs=-1,\\\n",
    "                                    scoring ='neg_mean_squared_error')\n",
    "        model_search.fit(X_train_scaled, y_train)\n",
    "        best_model = model_search.best_estimator_\n",
    "        return best_model\n",
    "    elif m in ['RF','GBR']:\n",
    "        model_search = GridSearchCV(model['modelobj'], model['params'],verbose=0, n_jobs=-1,\\\n",
    "                                    scoring ='neg_mean_squared_error')\n",
    "        model_search.fit(X_train_scaled, y_train)\n",
    "        best_model = model_search.best_estimator_\n",
    "        return best_model\n",
    "    else:\n",
    "        return model['modelobj']        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 2.2.1 Run Grid Search on all models to get initial best performing parameters **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding Best Params for :  LR\n",
      "Best Params Found for :  LR\n",
      "\n",
      "Finding Best Params for :  Ridge\n",
      "Best Params Found for :  Ridge\n",
      "\n",
      "Finding Best Params for :  DT\n",
      "Best Params Found for :  DT\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Get Model Params Using Grid Search CV\n",
    "\n",
    "def RunExpOnAllData(models,all_features,features_to_scale, alldata, sc = preprocessing.StandardScaler()):\n",
    "    for m in models:\n",
    "        print('Finding Best Params for : ', m)\n",
    "        models[m]['modelobj'] = GetBestParams(m, models[m], alldata, all_features,features_to_scale, 5, sc)\n",
    "        print('Best Params Found for : ', m)\n",
    "        print()\n",
    "\n",
    "\n",
    "RunExpOnAllData(models, all_features,features_to_scale, train_full_encoded, scalermain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 2.2.2  Evaluate all models to see what perfoms best on differnt folds created in 1.4 **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "code_folding": [
     0,
     5
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>Avg_Training_Time</th>\n",
       "      <th>Avg_Testing_Time</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model Evaluated</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>DT</th>\n",
       "      <td>422.007827</td>\n",
       "      <td>20.542790</td>\n",
       "      <td>16.749869</td>\n",
       "      <td>0.070327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LR</th>\n",
       "      <td>394.559408</td>\n",
       "      <td>19.863494</td>\n",
       "      <td>5.624955</td>\n",
       "      <td>0.079952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ridge</th>\n",
       "      <td>394.556857</td>\n",
       "      <td>19.863429</td>\n",
       "      <td>1.668606</td>\n",
       "      <td>0.057159</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        MSE       RMSE  Avg_Training_Time  Avg_Testing_Time\n",
       "Model Evaluated                                                            \n",
       "DT               422.007827  20.542790          16.749869          0.070327\n",
       "LR               394.559408  19.863494           5.624955          0.079952\n",
       "Ridge            394.556857  19.863429           1.668606          0.057159"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find model performance vased on k-fold data created earliers ( See 1.4)\n",
    "import multiprocessing as mp\n",
    "\n",
    "\n",
    "\n",
    "def RunExpOnSplitData(kfolddata, models, all_features,features_to_scale, sc = preprocessing.StandardScaler()):\n",
    "    for m in models:\n",
    "        pool = mp.Pool(mp.cpu_count() -1 )\n",
    "        #MSE_RMSE_results = pool.starmap(EvalModel, [ (models[m], kfolddata[curr_fold], train_features) for curr_fold in kfolddata])\n",
    "        MSE_RMSE_results = pool.starmap_async(EvalModelSplitData, [(m, models[m], kfolddata[curr_fold], all_features,features_to_scale, sc) for curr_fold in kfolddata]).get()\n",
    "        pool.close()\n",
    "    \n",
    "        models[m]['MSE'] = sum([i[0] for i in MSE_RMSE_results])/len(MSE_RMSE_results)\n",
    "        models[m]['RMSE'] = sum([i[1] for i in MSE_RMSE_results])/len(MSE_RMSE_results)\n",
    "        models[m]['Avg_Training_Time'] = sum([i[2] for i in MSE_RMSE_results])/len(MSE_RMSE_results)\n",
    "        models[m]['Avg_Testing_Time'] = sum([i[3] for i in MSE_RMSE_results])/len(MSE_RMSE_results)\n",
    "        \n",
    "        #print(m, models[m]['MSE'], models[m]['RMSE'],models[m]['Avg_Training_Time'],models[m]['Avg_Testing_Time'], k)              \n",
    "\n",
    "k = 10\n",
    "kfolddata =  GetKFoldData(train_full_encoded, k)    \n",
    "RunExpOnSplitData(kfolddata, models, all_features,features_to_scale, scalermain)\n",
    "models_df =  pd.DataFrame.from_dict(models, orient='index')\n",
    "models_df.index.name = 'Model Evaluated'\n",
    "models_df[['MSE','RMSE','Avg_Training_Time','Avg_Testing_Time']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Based on above resuts the  *Ridge* regression model will be used for further experiments in this notebook **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 2.2.3  Choose the model to be used **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge  is the model choosen\n"
     ]
    }
   ],
   "source": [
    "#Choose Model Having the least RMSE on average over k-fold data\n",
    "modelchosen = ('LR',models['LR'])\n",
    "for m in models:\n",
    "    if models[m]['RMSE']<modelchosen[1]['RMSE']:\n",
    "        modelchosen = (m,models[m])\n",
    "\n",
    "print(modelchosen[0], ' is the model choosen')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Feature Importance w.r.t best Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the best performing model I have tried to measure feature importance based on the impact they cause when the feature has been removed. (I have removed the features from the original category and not the encoded ones)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Features whose importance is to be checked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['companyId',\n",
       " 'jobType',\n",
       " 'degree',\n",
       " 'major',\n",
       " 'industry',\n",
       " 'yearsExperience',\n",
       " 'milesFromMetropolis']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Features whose importance will be evaluated\n",
    "org_features = org_features[1:]\n",
    "org_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Remove each feature at a time and compute/display the percentage increase in RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Check change in RMSE performance on k-fold data when a feature is excluded for the best performig model\n",
    "feature_imp_results = {}\n",
    "\n",
    "# IF required feature importance can be checked over all the models also\n",
    "#for m in models:\n",
    "#    modelchosen = (m, models[m])\n",
    "\n",
    "for f in org_features:\n",
    "    try:\n",
    "\n",
    "        all_features_wo_curr_f = [of for of in all_features if f not in of]\n",
    "        features_to_scale_wo_curr_f = [of for of in features_to_scale if f not in of]         \n",
    "\n",
    "        pool = mp.Pool(mp.cpu_count() -1 )\n",
    "        mse_rmse_results = pool.starmap_async(EvalModelSplitData, [(modelchosen[0], modelchosen[1], kfolddata[curr_fold], all_features_wo_curr_f, features_to_scale_wo_curr_f, scalermain) \\\n",
    "                                                                   for curr_fold in kfolddata]).get()\n",
    "        pool.close()\n",
    "        if f not in feature_imp_results:\n",
    "            feature_imp_results[f] = {}\n",
    "            feature_imp_results[f]['Model'] = modelchosen[0]\n",
    "            feature_imp_results[f]['MSE'] = sum([i[0] for i in mse_rmse_results])/len(mse_rmse_results)\n",
    "            feature_imp_results[f]['RMSE'] = sum([i[1] for i in mse_rmse_results])/len(mse_rmse_results)\n",
    "            feature_imp_results[f]['ORG_RMSE'] = modelchosen[1]['RMSE']\n",
    "            feature_imp_results[f]['Percentage_increase_RMSE'] = 100 * (feature_imp_results[f]['RMSE'] - modelchosen[1]['RMSE'])/modelchosen[1]['RMSE']\n",
    "    except:\n",
    "        print('Exception occured while evlaluation importance of feature', f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>MSE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>ORG_RMSE</th>\n",
       "      <th>Percentage_increase_RMSE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Feature Removed</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>jobType</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>774.525356</td>\n",
       "      <td>27.830257</td>\n",
       "      <td>19.863429</td>\n",
       "      <td>40.108015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yearsExperience</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>604.833805</td>\n",
       "      <td>24.593344</td>\n",
       "      <td>19.863429</td>\n",
       "      <td>23.812174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>milesFromMetropolis</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>527.666012</td>\n",
       "      <td>22.970960</td>\n",
       "      <td>19.863429</td>\n",
       "      <td>15.644483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>industry</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>524.698580</td>\n",
       "      <td>22.906255</td>\n",
       "      <td>19.863429</td>\n",
       "      <td>15.318731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>degree</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>414.598090</td>\n",
       "      <td>20.361645</td>\n",
       "      <td>19.863429</td>\n",
       "      <td>2.508205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>major</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>411.545053</td>\n",
       "      <td>20.286557</td>\n",
       "      <td>19.863429</td>\n",
       "      <td>2.130182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>companyId</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>394.527069</td>\n",
       "      <td>19.862680</td>\n",
       "      <td>19.863429</td>\n",
       "      <td>-0.003775</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Model         MSE       RMSE   ORG_RMSE  \\\n",
       "Feature Removed                                                \n",
       "jobType              Ridge  774.525356  27.830257  19.863429   \n",
       "yearsExperience      Ridge  604.833805  24.593344  19.863429   \n",
       "milesFromMetropolis  Ridge  527.666012  22.970960  19.863429   \n",
       "industry             Ridge  524.698580  22.906255  19.863429   \n",
       "degree               Ridge  414.598090  20.361645  19.863429   \n",
       "major                Ridge  411.545053  20.286557  19.863429   \n",
       "companyId            Ridge  394.527069  19.862680  19.863429   \n",
       "\n",
       "                     Percentage_increase_RMSE  \n",
       "Feature Removed                                \n",
       "jobType                             40.108015  \n",
       "yearsExperience                     23.812174  \n",
       "milesFromMetropolis                 15.644483  \n",
       "industry                            15.318731  \n",
       "degree                               2.508205  \n",
       "major                                2.130182  \n",
       "companyId                           -0.003775  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Display how much is the increase in error when a feature is dropped\n",
    "feature_imp_results_df = pd.DataFrame.from_dict(feature_imp_results, orient='index')\n",
    "feature_imp_results_df.index.name = 'Feature Removed'\n",
    "feature_imp_results_df.sort_values(by='Percentage_increase_RMSE', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** From above resuts it is evident that *JobType* has the most predictive power whereas the feature *companyId* has the least predicitve power **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# feature_imp = feature_imp_results_df.groupby('Model')\n",
    "# feature_imp.sort_values(by='percentage_error_increase', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Predict Salaries for Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A prediction of salaries is made for the test data based on the best model chosen above. The test data has been already been encoded in the same way as before and the scaling of numerica features is same as performed with the trianing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Predict Salaries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Helper Function to Predict Salaries Given testdata, model and scaling scheme\n",
    "def PredictSalary(m, model, train_full, test_data, all_features, features_to_scale, \\\n",
    "                       scaler = preprocessing.StandardScaler()):\n",
    "        \n",
    "    #train_model_full_data\n",
    "    X_train = train_full[all_features]\n",
    "    X_train_scaled = X_train\n",
    "    X_train_scaled[features_to_scale] = scaler.fit_transform(X_train[features_to_scale])\n",
    "    y_train = train_full['salary'] \n",
    "    \n",
    "    tr_st = time.time()\n",
    "    trained_model = model['modelobj'].fit(X_train_scaled, y_train)\n",
    "    tr_time = time.time() - tr_st \n",
    "    \n",
    "    #Test\n",
    "    X_test = test_data[all_features]\n",
    "    X_test_scaled = X_test\n",
    "    X_test_scaled[features_to_scale] = scaler.fit_transform(test_data[features_to_scale])\n",
    "    \n",
    "    tst_st = time.time()\n",
    "    test_predictions = trained_model.predict(X_test_scaled)\n",
    "    tst_time = time.time() - tst_st\n",
    "    \n",
    "    test_data['salary'] = test_predictions \n",
    "    \n",
    "    return test_data\n",
    "\n",
    "def get_currtime_str():\n",
    "    from datetime import datetime\n",
    "    \"\"\" Gets the current time in a particular format  \"\"\"\n",
    "\n",
    "    timestampformat = '%Y%m%d__%H%M%S'\n",
    "    currtime_str = str(datetime.now().strftime(timestampformat))\n",
    "    return currtime_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Predict Salaries of Test Data Based on Chosen Model and save the predictions in a csv file\n",
    "test_salaries = PredictSalary(modelchosen[0], modelchosen[1],train_full_encoded, test_data_features_encoded, all_features, features_to_scale, \\\n",
    "                          scalermain)\n",
    "assert(test_salaries.shape[0]==test_data_features_encoded.shape[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Save predicted salaries to csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Predicted Salaries for test data saved in file  20190326__173750_test_salaries.csv\n"
     ]
    }
   ],
   "source": [
    "currtime = get_currtime_str()\n",
    "test_salaries[['jobId','salary']].to_csv(currtime+'_test_salaries.csv', index = False)\n",
    "test_data_features_encoded.to_csv('test_full.csv', index = False)\n",
    "print(\" Predicted Salaries for test data saved in file \", currtime+'_test_salaries.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Estimating RMSE of Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Sample some of the test datat and then get similarity betweeen test data sample and training data jobs\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics.pairwise import *\n",
    "from scipy import spatial\n",
    "import scipy \n",
    "\n",
    "\n",
    "samplesize = 500\n",
    "#Get Scaled Data\n",
    "train_full_encoded_scaled = train_full_encoded\n",
    "test_data_features_encoded_scaled_sample = test_data_features_encoded.sample(n=samplesize, random_state=0)\n",
    "\n",
    "\n",
    "train_full_encoded_scaled[features_to_scale] = scalermain.fit_transform(train_full_encoded_scaled[features_to_scale])\n",
    "test_data_features_encoded_scaled_sample[features_to_scale] = \\\n",
    "                        scalermain.fit_transform(test_data_features_encoded_scaled_sample[features_to_scale])\n",
    "\n",
    "\n",
    "#Create Sparse Matrices of Train and Test Data\n",
    "TRNG_MAT = scipy.sparse.csr_matrix(train_full_encoded_scaled[all_features].values)\n",
    "TST_MAT = scipy.sparse.csr_matrix(test_data_features_encoded_scaled_sample[all_features].values)\n",
    "sim = cosine_similarity(TST_MAT, TRNG_MAT)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the index of most similar row in Training data w.r.t each row in test data sample\n",
    "max_idx_list = np.argmax(sim, axis=1).tolist()\n",
    "test_data_features_encoded_scaled_sample['most_similar_row'] = max_idx_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set the salary of each row in test data sample as equal to the salary of most similar row in traning_data\n",
    "def getSalFromTrainData(rowidx,trdata):\n",
    "    return trdata.iloc[rowidx]['salary']\n",
    "\n",
    "test_data_features_encoded_scaled_sample['most_similar_salary'] =\\\n",
    "    test_data_features_encoded_scaled_sample['most_similar_row'].apply(getSalFromTrainData, args=( train_full_encoded,))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21.12760235266107"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GET MSE for each row in test data sample as well as RMSE for whole sampled data\n",
    "def computeMSE(r):\n",
    "    return MSE([r['salary']], [r['most_similar_salary']])\n",
    "    #return MSE(x['salary'], x['most_similar_salary'])\n",
    "\n",
    "\n",
    "test_data_features_encoded_scaled_sample['MSE'] = \\\n",
    "                    test_data_features_encoded_scaled_sample.apply(computeMSE, axis =1)\n",
    "pred_rmse = test_data_features_encoded_scaled_sample['MSE'].mean() ** 0.5\n",
    "pred_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Based on 500 samples The estimated RMSE on the test data set should be in the range \n",
      " (19.694215763965044, 22.46973581924649) with a confidence_level of 0.95\n"
     ]
    }
   ],
   "source": [
    "# Estimate MSE Range based on MSE for each row in sample and based on that RMSE range\n",
    "from scipy.stats import sem as stderr\n",
    "from scipy.stats import t as tscore\n",
    "from scipy import mean\n",
    "\n",
    "MSEList = test_data_features_encoded_scaled_sample['MSE'].tolist()\n",
    "\n",
    "confidence_level = 0.95\n",
    "no_of_data_points = len(MSEList)\n",
    "mu = mean(MSEList)\n",
    "std_err = stderr(MSEList)\n",
    "error_range = std_err * tscore.ppf((1 + confidence_level) / 2, no_of_data_points - 1)\n",
    "error_range\n",
    "\n",
    "est_MSERange = (mu-error_range, mu + error_range)\n",
    "est_RMSERange = ((mu-error_range)**0.5, (mu+error_range)**0.5)\n",
    "est_RMSERange\n",
    "print(\" Based on \" + str(no_of_data_points) +\" samples The estimated RMSE on the test data set should be in the range \\n \" + \\\n",
    "      str(est_RMSERange) + \" with a confidence_level of \" + str(confidence_level))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
